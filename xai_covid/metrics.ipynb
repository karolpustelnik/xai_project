{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation with metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading imports, model and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciejpioro/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from models.resnetv2 import ResNet50\n",
    "from models.resnet_attention import resnet50 as ResNet50Att, ResNetAtt\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from melanoma.melanoma_loader import Melanoma_loader as melanoma_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import quantus\n",
    "\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.torchvision import ResNetCanonizer\n",
    "from zennit.attribution import Gradient\n",
    "from crp.image import imgify\n",
    "from captum.metrics import infidelity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "ROOT = \"data/train/train/\"\n",
    "\n",
    "model_paths = [\n",
    "    \"./model_resnet.pt\",\n",
    "    \"./model_resnet_attention.pt\",\n",
    "    \"./model_resnet_unbiased.pt\",\n",
    "    \"./model_resnet_attention_unbiased.pt\",\n",
    "]\n",
    "model_names = [\n",
    "    \"Biased Resnet\",\n",
    "    \"Biased Resnet+att\",\n",
    "    \"Unbiased Resnet\",\n",
    "    \"Unbiased Resnet+att\",\n",
    "]\n",
    "model_classes = [ResNet50, ResNet50Att, ResNet50, ResNet50Att]\n",
    "model_urls = [\n",
    "    \"https://www.dropbox.com/s/wyma5jispzl63gr/resnet_bias_ckpt_epoch_49.pth?dl=0\",\n",
    "    \"https://www.dropbox.com/s/wnvxz05hy2slymx/resnet_att_bias_ckpt_epoch_71.pth?dl=0\",\n",
    "    \"https://www.dropbox.com/s/1apem4cqx7akycq/resnet_unb_ckpt_epoch_23.pth?dl=0\",\n",
    "    \"https://www.dropbox.com/s/xl826hxrcrypon0/resnet_att_unb_ckpt_epoch_72.pth?dl=0\",\n",
    "]\n",
    "\n",
    "models = []\n",
    "for model_path, model_url, model_name, model_class in zip(\n",
    "    model_paths, model_urls, model_names, model_classes\n",
    "):\n",
    "    if not os.path.exists(model_path):\n",
    "        os.system(f\"wget -O {model_path} {model_url}\")\n",
    "\n",
    "    if model_class == ResNet50:\n",
    "        model = model_class(out_features=2, freeze=True, in_channels=3)\n",
    "    elif model_class == ResNet50Att:\n",
    "        model = model_class(pretrained=False)\n",
    "\n",
    "    model.load_state_dict(\n",
    "        torch.load(model_path, map_location=\"cpu\")[\"model\"], strict=False\n",
    "    )\n",
    "    model.eval()\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stacked_img(img_path, extra_img):\n",
    "    img = Image.open(ROOT + img_path + \".jpg\")\n",
    "\n",
    "    new_im = Image.new('RGB', (2 * WIDTH, HEIGHT))\n",
    "    new_im.paste(img, (0, 0))\n",
    "    new_im.paste(extra_img, (WIDTH, 0))\n",
    "    return new_im\n",
    "\n",
    "def iterate_class(dataset, find_melanoma=1):\n",
    "    for idx in range(len(dataset)):\n",
    "        if dataset[idx][1] == find_melanoma:\n",
    "            yield dataset[idx][0].view(1, 3, 256, 256), dataset.lookup_path(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataset = melanoma_dataset(root = \"data/train/train\", ann_path = \"melanoma/data/test_set.csv\", \n",
    "                          transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Resize((256, 256)),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])]))\n",
    "dataloader = data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "positive_iterator = iterate_class(dataset, find_melanoma=1)\n",
    "negative_iterator = iterate_class(dataset, find_melanoma=0)\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "def lrp_explainer(\n",
    "    inputs,\n",
    "    targets,\n",
    "    model,\n",
    "    abs=False,\n",
    "    normalise=False,\n",
    "    sum_channels=False,\n",
    "    *args,\n",
    "    **kwargs\n",
    ") -> np.array:\n",
    "    model.eval()\n",
    "    if isinstance(inputs, tuple) and len(inputs) == 1:\n",
    "        inputs = inputs[0]\n",
    "    if not isinstance(inputs, torch.Tensor):\n",
    "        inputs = (\n",
    "            torch.Tensor(inputs)\n",
    "            .reshape(\n",
    "                -1,\n",
    "                kwargs.get(\"nr_channels\", 3),\n",
    "                kwargs.get(\"img_size\", 256),\n",
    "                kwargs.get(\"img_size\", 256),\n",
    "            )\n",
    "            .to(kwargs.get(\"device\", None))\n",
    "        )\n",
    "    inputs.requires_grad = True\n",
    "    if not isinstance(targets, torch.Tensor):\n",
    "        targets = torch.as_tensor(targets).long().to(kwargs.get(\"device\", None))\n",
    "\n",
    "    assert (\n",
    "        len(np.shape(inputs)) == 4\n",
    "    ), \"Inputs should be shaped (nr_samples, nr_channels, img_size, img_size) e.g., (1, 3, 256, 256).\"\n",
    "\n",
    "    # use the ResNet-specific canonizer\n",
    "    canonizer = ResNetCanonizer()\n",
    "\n",
    "    # create a composite, specifying the canonizers\n",
    "    composite = EpsilonPlusFlat(canonizers=[canonizer])\n",
    "\n",
    "    col1 = targets == 0\n",
    "    col2 = targets == 1\n",
    "    target = torch.vstack([col1, col2]).T.float()\n",
    "\n",
    "    # create the attributor, specifying model and composite\n",
    "    if isinstance(model, ResNetAtt):\n",
    "        model = AttentionWrapper(model)\n",
    "\n",
    "    model.requires_grad_(True)\n",
    "    inputs.requires_grad_(True)\n",
    "    target.requires_grad_(True)\n",
    "\n",
    "    with Gradient(model=model, composite=composite) as attributor:\n",
    "        # compute the model output and attribution\n",
    "        output, attribution = attributor(inputs, target)\n",
    "\n",
    "    if abs:\n",
    "        attribution = torch.abs(attribution)\n",
    "\n",
    "    # sum over the channels\n",
    "    if sum_channels:\n",
    "        relevance = attribution.sum(1)\n",
    "    else:\n",
    "        relevance = attribution\n",
    "\n",
    "    explanation = relevance\n",
    "\n",
    "    if isinstance(explanation, torch.Tensor):\n",
    "        explanation = explanation.cpu().detach().numpy()\n",
    "\n",
    "    if normalise:\n",
    "        explanation = quantus.normalise_func.normalise_by_max(explanation)\n",
    "        \n",
    "    return explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m     targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m model, model_name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(models, model_names):\n\u001b[1;32m     13\u001b[0m     \u001b[39m# attention\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     explanation \u001b[39m=\u001b[39m lrp_explainer(\n\u001b[1;32m     15\u001b[0m         inputs\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m     16\u001b[0m         targets\u001b[39m=\u001b[39;49mtargets,\n\u001b[1;32m     17\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     18\u001b[0m         sum_channels\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m         normalise\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     img \u001b[39m=\u001b[39m imgify(explanation[\u001b[39m0\u001b[39m], symmetric\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcoldnhot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m     new_img \u001b[39m=\u001b[39m stacked_img(path, img)\n",
      "Cell \u001b[0;32mIn[5], line 61\u001b[0m, in \u001b[0;36mlrp_explainer\u001b[0;34m(inputs, targets, model, abs, normalise, sum_channels, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m target\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[39mwith\u001b[39;00m Gradient(model\u001b[39m=\u001b[39mmodel, composite\u001b[39m=\u001b[39mcomposite) \u001b[39mas\u001b[39;00m attributor:\n\u001b[1;32m     60\u001b[0m     \u001b[39m# compute the model output and attribution\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     output, attribution \u001b[39m=\u001b[39m attributor(inputs, target)\n\u001b[1;32m     63\u001b[0m \u001b[39m# sum over the channels\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mif\u001b[39;00m sum_channels:\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/zennit/attribution.py:180\u001b[0m, in \u001b[0;36mAttributor.__call__\u001b[0;34m(self, input, attr_output)\u001b[0m\n\u001b[1;32m    177\u001b[0m     attr_output_fn \u001b[39m=\u001b[39m attr_output\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomposite \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomposite\u001b[39m.\u001b[39mhandles:\n\u001b[0;32m--> 180\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39minput\u001b[39;49m, attr_output_fn)\n\u001b[1;32m    182\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39minput\u001b[39m, attr_output_fn)\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/zennit/attribution.py:226\u001b[0m, in \u001b[0;36mGradient.forward\u001b[0;34m(self, input, attr_output_fn)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    225\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39minput\u001b[39m)\n\u001b[0;32m--> 226\u001b[0m gradient, \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad((output,), (\u001b[39minput\u001b[39;49m,), grad_outputs\u001b[39m=\u001b[39;49m(attr_output_fn(output\u001b[39m.\u001b[39;49mdetach()),))\n\u001b[1;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m output, gradient\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/zennit/core.py:193\u001b[0m, in \u001b[0;36mHook.pre_forward.<locals>.wrapper\u001b[0;34m(grad_input, grad_output)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward)\n\u001b[1;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(grad_input, grad_output):\n\u001b[0;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m hook_ref()\u001b[39m.\u001b[39;49mbackward(module, grad_input, hook_ref()\u001b[39m.\u001b[39;49mstored_tensors[\u001b[39m'\u001b[39;49m\u001b[39mgrad_output\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/zennit/core.py:337\u001b[0m, in \u001b[0;36mBasicHook.backward\u001b[0;34m(self, module, grad_input, grad_output)\u001b[0m\n\u001b[1;32m    335\u001b[0m     inputs\u001b[39m.\u001b[39mappend(\u001b[39minput\u001b[39m)\n\u001b[1;32m    336\u001b[0m     outputs\u001b[39m.\u001b[39mappend(output)\n\u001b[0;32m--> 337\u001b[0m gradients \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(outputs, inputs, grad_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_mapper(grad_output[\u001b[39m0\u001b[39;49m], outputs))\n\u001b[1;32m    338\u001b[0m \u001b[39m# relevance = self.reducer([input.detach() for input in inputs], [gradient.detach() for gradient in gradients])\u001b[39;00m\n\u001b[1;32m    339\u001b[0m relevance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreducer(inputs, gradients)\n",
      "File \u001b[0;32m~/Documents/studia/xai/xai_project/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:300\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[1;32m    299\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    301\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[1;32m    302\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "POSITIVE = False\n",
    "\n",
    "for i in range(100):\n",
    "    print(i)\n",
    "    if POSITIVE:\n",
    "        x, path = next(positive_iterator)\n",
    "        targets = [1] * x.shape[0]\n",
    "    else:\n",
    "        x, path = next(negative_iterator)\n",
    "        targets = [0] * x.shape[0]\n",
    "\n",
    "    for model, model_name in zip(models, model_names):\n",
    "        # attention\n",
    "        explanation = lrp_explainer(\n",
    "            inputs=x,\n",
    "            targets=targets,\n",
    "            model=model,\n",
    "            sum_channels=True,\n",
    "            normalise=True\n",
    "        )\n",
    "        img = imgify(explanation[0], symmetric=True, cmap='coldnhot')\n",
    "        new_img = stacked_img(path, img)\n",
    "        os.makedirs(f'lrp_results/{int(POSITIVE)}/{path}/', exist_ok=True)\n",
    "        new_img.save(f'lrp_results/{int(POSITIVE)}/{path}/{model_name}.png')\n",
    "        # show the image\n",
    "        # display(new_img)\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate AvgSensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Resnet avg sensitivity: 0.013954376486944966\n",
      "Biased Resnet+att avg sensitivity: 0.04411492634098976\n",
      "Unbiased Resnet avg sensitivity: 0.02628094685147516\n",
      "Unbiased Resnet+att avg sensitivity: 0.05559212098130957\n"
     ]
    }
   ],
   "source": [
    "# x, y = next(iter(dataloader))\n",
    "from itertools import repeat, chain\n",
    "\n",
    "# x, _ = next(positive_iterator)\n",
    "# y = torch.tensor([1])\n",
    "metric_init = quantus.AvgSensitivity(\n",
    "    nr_samples=10,\n",
    "    lower_bound=0.1,\n",
    "    norm_numerator=quantus.norm_func.fro_norm,\n",
    "    norm_denominator=quantus.norm_func.fro_norm,\n",
    "    perturb_func=quantus.perturb_func.uniform_noise,\n",
    "    similarity_func=quantus.similarity_func.difference,\n",
    "    disable_warnings=True,\n",
    "    normalise=True,\n",
    "    abs=True,\n",
    ")\n",
    "\n",
    "SAMPLE_SIZE = 10\n",
    "\n",
    "positives = []\n",
    "negatives = []\n",
    "for i in range(SAMPLE_SIZE):\n",
    "    x, _ = next(positive_iterator)\n",
    "    positives.append(x)\n",
    "    x, _ = next(negative_iterator)\n",
    "    negatives.append(x)\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "    s = 0 \n",
    "    for sample, target in zip(\n",
    "        chain(positives, negatives),\n",
    "        chain(repeat(1, SAMPLE_SIZE), repeat(0, SAMPLE_SIZE)),\n",
    "    ):\n",
    "        value = metric_init(\n",
    "            model=model,\n",
    "            x_batch=sample.numpy(),\n",
    "            y_batch=np.array([target]),\n",
    "            explain_func=lrp_explainer,\n",
    "        )\n",
    "        s += value[0]\n",
    "    print(f\"{model_name} avg sensitivity: {s / (2 * SAMPLE_SIZE)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Infidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biased Resnet infidelity: tensor([0.0480])\n",
      "Biased Resnet+att infidelity: tensor([0.0731])\n",
      "Unbiased Resnet infidelity: tensor([0.0808])\n",
      "Unbiased Resnet+att infidelity: tensor([0.1556])\n"
     ]
    }
   ],
   "source": [
    "def perturb_fn(inputs):\n",
    "   noise = torch.tensor(np.random.normal(0, 0.01, inputs.shape)).float()\n",
    "   return noise, inputs - noise\n",
    "\n",
    "for model, model_name in zip(models, model_names):\n",
    "   if isinstance(model, ResNetAtt):\n",
    "      model = AttentionWrapper(model)\n",
    "   \n",
    "   s = 0\n",
    "   for sample, target in zip(\n",
    "      chain(positives, negatives),\n",
    "      chain(repeat(1, SAMPLE_SIZE), repeat(0, SAMPLE_SIZE)),\n",
    "   ):\n",
    "      explanation = lrp_explainer(sample, torch.tensor([target]), model, abs=False, normalise=True, sum_channels=False)\n",
    "\n",
    "      infid = infidelity(model, perturb_fn, x, torch.tensor(explanation, requires_grad=True), target=target)\n",
    "      s += infid\n",
    "   print(f'{model_name} infidelity: {s / (2 * SAMPLE_SIZE)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4c2c4e7a89c5b573166a4489afe541dfe8b87c3274e8423ff96c5198ec1ec65b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
